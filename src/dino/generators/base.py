import sys
import os
import subprocess
import logging, traceback
from optparse import OptionParser
from os.path import join as pjoin
import yaml

import shutil
try:
    import hashlib
except ImportError:
    import md5 as hashlib

import sqlalchemy.exc as sa_exc

from dino.exception import DinoException

from dino.db import DbConfig

class GeneratorException(DinoException):
    pass

class NoSuchGeneratorError(GeneratorException):
    pass

class GeneratorQueryError(GeneratorException):
    pass

class GeneratorExecutionError(GeneratorException):
    pass

try:
    check_call = subprocess.check_call
except AttributeError:
    # Raided straight fom /usr/lib/python2.5/subprocess.py
    def check_call(*popenargs, **kwargs):
        """Run command with arguments.  Wait for command to complete.  If
        the exit code was zero then return, otherwise raise
        CalledProcessError.  The CalledProcessError object will have the
        return code in the returncode attribute.

        The arguments are the same as for the Popen constructor.  Example:

        check_call(["ls", "-l"])
        """
        retcode = subprocess.call(*popenargs, **kwargs)
        cmd = kwargs.get("args")
        if cmd is None:
            cmd = popenargs[0]
        if retcode:
            raise RuntimeError(retcode, cmd)
        return retcode



class GeneratorMeta(type):
    def __init__(cls, name, bases, dict_):
        super(GeneratorMeta, cls).__init__(name, bases, dict_)

        cls.log = logging.getLogger("dino.generate." + name)

        if bases[0] == object:
            cls.MAP = {}
            return

        if not hasattr(cls, 'NAME'):
            raise Exception("Subclass must define NAME: %s" % name)

        cls.activate = cls.activate_decorator(cls.activate)

        if cls.NAME is None:
            return
        else:
            cls.MAP[cls.NAME] = cls

    def activate_decorator(cls, fn):
        def new_activate(self):
            if not int(self.settings.disable_activate):
                return fn(self)
            else:
                self.log.info("Activate is disabled")
                return None

        new_activate.__doc__ = fn.__doc__
        new_activate.__name__ = fn.__name__
        return new_activate

class Generator(object):
    __metaclass__ = GeneratorMeta

    PROG_NAME = "dino"

    #
    # Default/Abstract Instance Methods
    #
    def __init__(self, db_config, generator_opts):
        self.db_config = db_config
        self.settings = generator_opts
        self.workdir = pjoin(self.settings.workdir, self.NAME)

    @classmethod
    def find_generator_class(cls, name):
        if name in cls.MAP:
            return cls.MAP[name]
        else:
            return None
    @classmethod
    def get_generator_class(cls, name):
        g = cls.find_generator_class(name)
        if g is not None:
            return g
        else:
            raise NoSuchGeneratorError(name)

    @classmethod
    def generator_class_iterator(cls, exclude=()):
        if not isinstance(exclude, (list, tuple, set)):
            exclude = (exclude,)

        for g in cls.MAP.values():
            if g in exclude or g.NAME in exclude:
                continue
            yield g

    def parse(self, args):
        if hasattr(self, 'OPTIONS'):
            parser = OptionParser()
            assert isinstance(self.OPTIONS, (tuple, list)), "%s.OPTIONS must be a tuple or list" % self.__class__.__name__
            for opt in self.OPTIONS:
                parser.add_option(opt)
            (self.option, self.args) = parser.parse_args(args)
        else:
            self.option = None
            self.args = args


    def generate(self):
        '''Generate Config file(s) in temporary location'''

        raise NotImplementedError("generate")

    def compare(self):
        ''' Compare the Generated Config with the Currently Active Config and report what would
        change with activation'''
        pass

    def activate(self):
        '''Activate configuration generated by moving files and / or restarting processes'''
        raise NotImplementedError("activate")



    # helper methods for (un)setting a lock during generation and activation
    # of the various services
    def lock(self, pname):
        lock_dir = self.settings.lock_dir
        if not os.path.exists(lock_dir):
            os.makedirs(lock_dir)
        lock_file = os.path.join(lock_dir, '%s.lock' % pname)
        # try to lock file - fail if you cannot
        try:
            fd = open(lock_file, 'w')
            fcntl.lockf(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            return fd
        except IOError, ex:
            if ex.errno == errno.EACCES or ex.errno == errno.EAGAIN:
                # lock could not be acquired - close fd and error out
                fd.close()
                self.log.error('%s is locked for operation: %s' % (lock_file, pname))
            else:
                self.log.error('could not open lock file %s' % lock_file)
            raise

    def unlock(self, pname, fd):
        lock_dir = self.settings.lock_dir
        if not os.path.exists(lock_dir):
            # somebody messed with var dirs - raise hell
            self.log.error('%s gone before unlock' % lock_dir)
            raise RuntimeError('%s gone before unlock' % lock_dir)

        lock_file = os.path.join(lock_dir, '%s.lock' % pname)
        if not os.path.exists(lock_file):
            # why oh why!!
            self.log.error('%s gone before unlock' % lock_file)
            raise RuntimeError('%s gone before unlock' % lock_file)
        # try to lock file - fail if you cannot
        try:
            fcntl.lockf(fd, fcntl.LOCK_UN)
            fd.close()
        except IOError, ex:
            self.log.error('cannot unlock previously locked %s' % lock_file)
            raise

    @staticmethod
    def setup_dir(dir, wipe=True, default_mode=0755):
        '''clean a directory for use as a dumping ground'''
        from shutil import rmtree
        if wipe and os.path.exists(dir):
            if os.path.isdir(dir):
                rmtree(dir)
            else:
                os.unlink(dir)
        # make the directory.  permissions?
        os.makedirs(dir, mode=default_mode)


    @staticmethod
    def check_call(*args, **kwargs):
        return check_call(*args, **kwargs)


    @staticmethod
    def pull_rapids_datacenter(settings, datacenter):
        fp = os.path.join(settings.rapids_root, 'release', 'datacenter', datacenter)
        fd = open(fp, 'r')

        return yaml.load(fd)['tmpl_data']

    @staticmethod
    def rsync_directory(src, trg, delete=True, verbose=True, extra_args=()):
        # rsync oddity.  ensure trailing slashes so that it doesn't try to 
        # inject it as a subdir
        src = src.rstrip('/') + '/'
        trg = trg.rstrip('/') + '/'
        args = ['rsync', src, trg, '-ruaHI']
        if verbose:
            args.append('-v')
        if delete:
            args.append('--delete')
        args.extend(extra_args)
        check_call(args)

    def sync_directory(self, source, target, delete=True, exclude=()):
        ''' 
            rsync-like copying with logging output
            source: source directory to sync
            target: target directory to sync
            delete: delete files that exist in target, but not in source.
            exclude: list of relative paths to exclude from the sync (relative to target)
        '''

        source_root = source.rstrip('/') + '/'
        target_root = target.rstrip('/') + '/'
        source_len = len(source_root)
        target_len = len(target_root)

        # clean out stuff that is not supposed to be there if necessary
        if delete:
            for (dir, dirnames, filenames) in os.walk(target_root):
                relative_path = dir[target_len:]

                for f in filenames:
                    if os.path.join(relative_path, f) in exclude:
                        continue

                    source_fp = os.path.join(source_root, relative_path, f)
                    target_fp = os.path.join(target_root, relative_path, f)

                    if not os.path.exists(source_fp):
                        self.log.info("   Removing: %s", target_fp)
                        os.unlink(target_fp)

        for (dir, dirnames, filenames) in os.walk(source_root):
            relative_path = dir[source_len:]

            # make sure target dir exists
            target_dir = os.path.join(target_root, relative_path)
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)

            # check all files
            for f in filenames:
                if os.path.join(relative_path, f) in exclude:
                        continue

                source_fp = os.path.join(source_root, relative_path, f)
                target_fp = os.path.join(target_root, relative_path, f)

                if os.path.islink(source_fp):
                    link_path = os.readlink(source_fp)

                    if os.path.exists(target_fp):
                        if not os.path.islink(target_fp):
                            self.log.info("   Removing (symlink): %s", target_fp)
                            os.unlink(target_fp)

                        elif link_path == os.readlink(target_fp):
                            continue

                    self.log.info("   Updating: (link) %s", target_fp)
                    os.symlink(link_path, target_fp)

                elif os.path.isfile(source_fp):
                    if os.path.exists(target_fp):

                        if not os.path.isfile(target_fp):
                            self.log.info("   Removing (non-file): %s", target_fp)
                            os.unlink(target_fp)

                        elif self.file_hash(source_fp) == self.file_hash(target_fp):
                            continue

                    self.log.info("   Updating: %s", target_fp)
                    shutil.copy2(source_fp, target_fp)

                else:
                    self.log.info("   Unknown object: %s", source_fp)



    def file_hash(self, filepath):
        f = open(filepath)
        data = f.read()
        f.close()
        return hashlib.md5(data).digest()


    @classmethod
    def main(cls):
        from cli import GeneratorCli
        return GeneratorCli(cls).main(sys.argv[1:])




